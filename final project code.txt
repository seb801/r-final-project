library("tidyverse")
install.packages("telegram.bot")
library("telegram.bot")

require(rvest)
# 날짜를 다루는 패키지
require(lubridate)

# 생성할 폴더 이름
folder <- 'c:/test'

# 폴더가 없으면 생성
if(!dir.exists(folder)) dir.create(folder)
   
# working directory 변경
setwd(folder)

# 오늘 날짜
date <- Sys.Date()

# 현재 시간
h <- hour(Sys.time())
m <- minute(Sys.time())

# '현재 시간' 폴더 이름 
now <- paste(date, h, m, sep='-')
now.folder <- paste(folder, now, sep='/')

# 폴더 생성   
if(!dir.exists(now.folder)) dir.create(now.folder)

setwd(now.folder)   

url1='https://www.menupan.com/restaurant/bestrest/bestrest.asp?trec=127&areacode=jl207&pt=wk&page='

page=1:2

pages=paste0(url1, page, sep='')

file.name <- paste0('page', page, '.txt')

for(i in 1:length(pages)){
  file <- read_html(pages[i],encoding='CP949') 
  # 웹 사이트에서 다운받은 페이지 저장
  write_xml(file, file = file.name[i])
}





install.packages("rvest")
library(rvest)
install.packages("RSelenium")
library(RSelenium)
library(XML)
install.packages("stringr")
library(stringr)



remDr <- remoteDriver(
  remoteServerAddr = "localhost",
  port = 4446L,
  browserName = "chrome"
)

urls='https://www.menupan.com/restaurant/bestrest/bestrest.asp?pt=wk&areacode=jl207'

remDr$open()

remDr$navigate(urls)

  # 전체 페이지 읽어오기
  htxt <- read_html(urls,encoding="CP949")
  
  #전체 페이지중 뽑아내고 싶은 부분 코드 쓰기  
  table <- html_nodes(htxt,'.list')
  
  #위에서 뽑은 리스트중에 안에 있는 내용 뽑아오기  
  contents <- table%>%html_nodes('li')
  
  #추출한 html 소스를 'html_text'를 이용해 텍스트로 저장. 저장 시 필요없는 특수문자(정규표현식: '\\W')를 'str_replace_all'을 이용하여 제거 (이 코드 쓰려면 stringr패키지 설치 필요)
  contents <- str_replace_all(html_text(contents), "\\W", " ")


  # 데이터 추출할때 list 코드가 겹쳐 필요없는 부분까지 같이 나타나므로 행렬로 만들어 필요 없는 부분 제거해 준다.

  contents<-matrix(contents,nrow=41)
  
  contents<-contents[-c(1:16),]

  contents
  
  url2='https://www.menupan.com/restaurant/bestrest/bestrest.asp?trec=127&areacode=jl207&pt=wk&page=2'
  
  remDr$open()
  
  remDr$navigate(url2)
  
  # 전체 페이지 읽어오기
  htxt <- read_html(url2,encoding="CP949")
  
  #전체 페이지중 뽑아내고 싶은 부분 코드 쓰기  
  table <- html_nodes(htxt,'.list')
  
  #위에서 뽑은 리스트중에 안에 있는 내용 뽑아오기  
  content2 <- table%>%html_nodes('li')
  
  #추출한 html 소스를 'html_text'를 이용해 텍스트로 저장. 저장 시 필요없는 특수문자(정규표현식: '\\W')를 'str_replace_all'을 이용하여 제거 (이 코드 쓰려면 stringr패키지 설치 필요)
  content2 <- str_replace_all(html_text(content2), "\\W", " ")
  
  
  # 데이터 추출할때 list 코드가 겹쳐 필요없는 부분까지 같이 나타나므로 행렬로 만들어 필요 없는 부분 제거해 준다.
  
  content2<-matrix(content2,nrow=41)
  
  content2<-content2[-c(1:16),]  
  
  content2
  
  
  
  
#명사추출
  
install.packages("rJava")
install.packages("memoise")
install.packages("KoNLP")
library(KoNLP)
library(dplyr)


nounc <- extractNoun(contents)
nounc

nounc1 <- extractNoun(content2)
nounc1



df_nounc <- NULL

for (i in 0:24){
  
  df_nounc_b <- as.data.frame(nounc[i+1])
  
  names(df_nounc_b) <- c('keywords')
  
  df_nounc <- rbind(df_nounc, df_nounc_b)
  
  
}

df_nounc1 <- NULL

for (i in 0:24){
  
  df_nounc1_b <- as.data.frame(nounc1[i+1])
  
  names(df_nounc1_b) <- c('keywords')
  
  df_nounc1 <- rbind(df_nounc1, df_nounc1_b)
  
}


word <-df_nounc
word

words1 <-df_nounc1
words1

keywords<-rbind(word,words1)

#통계처리(빈도확인-> 많이 나온 단어수대로 정렬해 이름과 나온수 기준으로 데이터 프레임 만든다.)

keywords$keywords <- as.character(keywords$keywords)

key <- keywords %>% filter(nchar(keywords)>=2 & nchar(keywords)<=9) %>% group_by(keywords) %>% summarise(n=n())

keywords <- key[order(-count_key$n), ]

head(keywords)


# 단어빈도수 워드클라우드로 시각화 하기
install.packages('wordcloud2')
library('wordcloud2')
wordcloud2(keywords)
wordcloud2(keywords, color='skyblue', minRotation=0, maxRotation=0)

# 의미연결망 그리기
install.packages('igraph')
library('igraph')

#igraph는 이름 그대로 (네트워크) 그래프를 다루는 패키지. 그래서 keywords에 담아둔 자료를 그래프 형태로 바꿔줘야 한다.
mg <- graph_from_data_frame(keywords)
mg

# 낱말과 문장 번호는 서로 성질이 다른 변수. 이런 식으로 정리한 그래프를 '이분(Bipartite) 그래프'라 부른다. 이분 그래프는 다음과 같은 과정을 거쳐서 분해
V(mg)$type <- bipartite_mapping(mg)$type
mm <- as_incidence_matrix(mg) %*% t(as_incidence_matrix(mg))
diag(mm) <- 0
mg <- graph_from_adjacency_matrix(mm)

plot(mg)


# 깔끔한 의미 연결망 그리기
install.packages('tidygraph')
install.packages('ggraph')
library('tidygraph')
library('ggraph')

mg %>% as_tbl_graph() %>%
  ggraph() +
  geom_edge_link(aes(start_cap = label_rect(node1.name), end_cap = label_rect(node2.name))) +
  geom_node_text(aes(label=name))


# 빈도수 이용한 산점도 그리기
library(ggplot2)
h=head(keywords)
x=c('전주','완산구','덕진구','29','고사','덕진동2')
y=c(50,34,15,4,4,4)
data=data.frame(x,y)
barplot(data)
ggplot(data, aes(x, y)) + geom_point()
